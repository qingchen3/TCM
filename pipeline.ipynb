{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "from MultipleHash import get_hashcodes\n",
    "from queue import PriorityQueue\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/qing/projects/TCM/datasets'\n",
    "data_file = 'dblp.xml'\n",
    "dtd_file = 'dblp.dtd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#parse DBLP to obtain co-author dataset\n",
    "\n",
    "dtd = ET.DTD(os.path.join(data_dir, dtd_file))  # read DTD\n",
    "# iterate through nodes\n",
    "\n",
    "records = []\n",
    "\n",
    "for event, element in ET.iterparse(os.path.join(data_dir, data_file), dtd_validation=True):\n",
    "    # print all children\n",
    "    coauthors = set()\n",
    "    for child in element:\n",
    "        if child.tag == 'author':\n",
    "            coauthors.add(child.text)\n",
    "        if child.tag == 'title':\n",
    "            title = child.text\n",
    "        if child.tag == 'year':\n",
    "            year = child.text\n",
    "\n",
    "    if len(coauthors) < 2:\n",
    "        continue\n",
    "        \n",
    "    records.append((title, coauthors, year))\n",
    "    \n",
    "print(len(records))\n",
    "count = 0\n",
    "output = []\n",
    "f = open('coauthors.json', 'w', encoding='utf-8')\n",
    "for record in records:\n",
    "    json_obj = dict()\n",
    "    json_obj['title'] = record[0]\n",
    "    json_obj['authors'] = list(record[1])\n",
    "    json_obj['year'] = record[2]\n",
    "    output.append(json_obj)\n",
    "    #json.dump(json_obj, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "output.sort(key=lambda tup: tup['year'])\n",
    "\n",
    "json.dump(output, f, ensure_ascii=False, indent=4)\n",
    "f.close()\n",
    "print(len(output))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def independent_ttest(data1, data2, alpha=0.05):\n",
    "\n",
    "    # calculate means\n",
    "    mean1, mean2 = mean(data1), mean(data2)\n",
    "    # calculate standard errors\n",
    "    std1, std2 = std(data1, ddof=1), std(data2, ddof=1)\n",
    "    \n",
    "    n1, n2 = len(data1), len(data2)\n",
    "    se1, se2 = std1/sqrt(n1), std2/sqrt(n2)\n",
    "    # standard error on the difference between the samples\n",
    "    sed = sqrt(se1**2.0 + se2**2.0)\n",
    "    # calculate the t statistic\n",
    "    t_stat = (mean1 - mean2) / sed\n",
    "    # degrees of freedom\n",
    "    df = len(data1) + len(data2) - 2\n",
    "    # calculate the critical value\n",
    "    cv = stats.t.ppf(1.0 - alpha, df)\n",
    "    # calculate the p-value\n",
    "    p = (1.0 - stats.t.cdf(abs(t_stat), df)) * 2.0\n",
    "    # return everything\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_reader = open('coauthors.json', 'r', encoding='utf-8')\n",
    "json_objects = json.load(json_reader)\n",
    "records = []\n",
    "for json_obj in json_objects:\n",
    "    records.append(json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current year: 1930\n",
      "current year: 1931\n",
      "current year: 1932\n",
      "current year: 1933\n",
      "current year: 1934\n",
      "current year: 1935\n",
      "current year: 1936\n",
      "current year: 1937\n",
      "current year: 1938\n",
      "current year: 1939\n",
      "current year: 1940\n",
      "current year: 1941\n",
      "current year: 1942\n",
      "current year: 1943\n",
      "current year: 1944\n",
      "current year: 1945\n",
      "current year: 1946\n",
      "current year: 1947\n",
      "current year: 1948\n",
      "current year: 1949\n",
      "current year: 1950\n",
      "current year: 1951\n",
      "current year: 1952\n",
      "current year: 1953\n",
      "current year: 1954\n",
      "current year: 1955\n",
      "current year: 1956\n",
      "current year: 1957\n",
      "current year: 1958\n",
      "current year: 1959\n",
      "current year: 1960\n",
      "current year: 1961\n",
      "current year: 1962\n",
      "current year: 1963\n",
      "current year: 1964\n",
      "current year: 1965\n",
      "current year: 1966\n",
      "current year: 1967\n",
      "current year: 1968\n",
      "current year: 1969\n",
      "current year: 1970\n",
      "current year: 1971\n",
      "current year: 1972\n",
      "current year: 1973\n",
      "current year: 1974\n",
      "current year: 1975\n",
      "current year: 1976\n",
      "current year: 1977\n",
      "current year: 1978\n",
      "current year: 1979\n",
      "current year: 1980\n",
      "current year: 1981\n",
      "current year: 1982\n",
      "current year: 1983\n",
      "current year: 1984\n",
      "current year: 1985\n",
      "current year: 1986\n",
      "current year: 1987\n",
      "current year: 1988\n",
      "current year: 1989\n",
      "current year: 1990\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2c832cc0af88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                             \u001b[0mtriadic_closure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_year\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mfrequency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_year = 1930\n",
    "pos = 0\n",
    "\n",
    "triadic_closure = set()\n",
    "frequency = dict()\n",
    "nodes_set = set()\n",
    "total_count = 0\n",
    "while pos < len(records) :\n",
    "    print('current year: %d' %current_year)    \n",
    "\n",
    "    #check triadic closure\n",
    "    while int(records[pos]['year']) == current_year:\n",
    "        authors = records[pos]['authors']\n",
    "        authors.sort()\n",
    "        for i in range(len(authors)):      \n",
    "            for j in range(i + 1, len(authors)):\n",
    "                nodes_set.add(authors[i])\n",
    "                nodes_set.add(authors[j])\n",
    "                if (authors[i], authors[j]) not in frequency:\n",
    "                    for node in nodes_set:\n",
    "                        if((node, authors[i]) in frequency or (authors[i], node) in frequency) and ((node, authors[j]) in frequency or (authors[j], node) in frequency):\n",
    "                            triadic_closure.add((node, authors[i], authors[j], current_year))\n",
    "                    frequency[(authors[i], authors[j])] = dict()\n",
    "                if current_year not in frequency[(authors[i], authors[j])]:\n",
    "                    frequency[(authors[i], authors[j])][current_year] = 0\n",
    "                frequency[(authors[i], authors[j])][current_year] += 1\n",
    "                \n",
    "        pos += 1\n",
    "    current_year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_year = 1930\n",
    "pos = 0\n",
    "\n",
    "strong_tie_set = set()\n",
    "weak_tie_set = set()\n",
    "\n",
    "connection_dict = dict()\n",
    "\n",
    "initial_list = [0, 0, 0, 0]\n",
    "threshold = 3\n",
    "time_window = 4 #we look back past 4 years\n",
    "\n",
    "statics_dict = dict()\n",
    "\n",
    "total_count = 0\n",
    "while pos < len(records):\n",
    "    print('current year: %d' %current_year)    \n",
    "    num_of_new_triadic_closure = 0\n",
    "    \n",
    "    #remove coauthor pairs that are not inactive during past 4 years\n",
    "    remove_coauthor_list = []\n",
    "    \n",
    "    #update connection dict\n",
    "    for coauthor_pair, value_list in connection_dict.items():\n",
    "        connection_dict[coauthor_pair] = connection_dict[coauthor_pair][1:]\n",
    "        connection_dict[coauthor_pair].append(0)\n",
    "        #connection_dict[coauthor_pair] -= 1\n",
    "        if sum(connection_dict[coauthor_pair]) == 0:\n",
    "            remove_coauthor_list.append(coauthor_pair)\n",
    "   \n",
    "    for coauthor_pair in remove_coauthor_list:\n",
    "        del connection_dict[coauthor_pair]\n",
    "    \n",
    "    #calculate triads from strong ties\n",
    "    triads = set()\n",
    "    strong_tie_list = list(strong_tie_set)\n",
    "    for i in range(len(strong_tie_list)):        \n",
    "        author_i_1, author_i_2 = strong_tie_list[i]\n",
    "        for j in range(i + 1, len(strong_tie_list)):\n",
    "            author_j_1, author_j_2 = strong_tie_list[j]\n",
    "            if author_i_1 == author_j_2 and author_i_2 == author_j_1:\n",
    "                continue\n",
    "            if author_i_1 == author_j_1 and (author_i_2, author_j_2) not in strong_tie_set: #author_i_1 is the triadic node\n",
    "                triads.add((author_i_1, author_i_2, author_j_2))\n",
    "                triads.add((author_i_1, author_j_2, author_i_2))\n",
    "            if author_i_1 == author_j_2 and (author_i_2, author_j_1) not in strong_tie_set:\n",
    "                triads.add((author_i_1, author_i_2, author_j_1))    \n",
    "                triads.add((author_i_1, author_j_1, author_i_2))                \n",
    "            if author_i_2 == author_j_1 and (author_i_1, author_j_2) not in strong_tie_set: #author_i_2 is the triadic node\n",
    "                triads.add((author_i_2, author_i_1, author_j_2))\n",
    "                triads.add((author_i_2, author_j_2, author_i_1))\n",
    "            if author_i_2 == author_j_2 and (author_i_1, author_j_1) not in strong_tie_set:\n",
    "                triads.add((author_i_2, author_i_1, author_j_1))\n",
    "                triads.add((author_i_2, author_j_1, author_i_1))\n",
    "                \n",
    "    #to count how many triadic_closures are formed for current year\n",
    "    current_triadic_closure_set = set() \n",
    "\n",
    "    #check triadic closure\n",
    "    while int(records[pos]['year']) == current_year:\n",
    "        authors = records[pos]['authors']\n",
    "        \n",
    "        for i in range(len(authors)):      \n",
    "            for j in range(i + 1, len(authors)):\n",
    "                for (parent, child1, child2) in triads:                    \n",
    "                    if authors[i] == child1 and authors[j] == child2:\n",
    "                        current_triadic_closure_set.add((authors[i], authors[j]))\n",
    "                        current_triadic_closure_set.add((authors[j], authors[i]))\n",
    "                \n",
    "                        if (authors[i], authors[j]) not in connection_dict or (authors[j], authors[i]) not in connection_dict:\n",
    "                            #new coauthorship\n",
    "                            #connection_dict[(authors[i], authors[j])] = initial_list\n",
    "                            #connection_dict[(authors[j], authors[i])] = initial_list\n",
    "                            num_of_new_triadic_closure += 1                        \n",
    "                        break\n",
    "                \n",
    "                if (authors[i], authors[j]) not in connection_dict or (authors[j], authors[i]) not in connection_dict:\n",
    "                        #new coauthorship\n",
    "                        connection_dict[(authors[i], authors[j])] = initial_list\n",
    "                        connection_dict[(authors[j], authors[i])] = initial_list\n",
    "                        \n",
    "                connection_dict[(authors[i], authors[j])][-1] = 1\n",
    "                connection_dict[(authors[j], authors[i])][-1] = 1\n",
    "                    \n",
    "        pos += 1\n",
    "    \n",
    "    strong_tie_set.clear()\n",
    "    weak_tie_set.clear()\n",
    "    #update strong ties and weak ties\n",
    "    for (author1, author2), score_list in connection_dict.items():\n",
    "        score = sum(score_list)\n",
    "        if score >= threshold: #strong tie\n",
    "                strong_tie_set.add((author1, author2))            \n",
    "                strong_tie_set.add((author2, author1))\n",
    "        else: #weak tie             \n",
    "                weak_tie_set.add((author1, author2))\n",
    "                weak_tie_set.add((author2, author1))\n",
    "                \n",
    "    print(\"%d triads in the previous year\" %(len(triads) // 2))\n",
    "    print(\"%d newly created triadic closure\" %num_of_new_triadic_closure)\n",
    "    print(\"%d triadic closure\" %(len(current_triadic_closure_set) // 2))\n",
    "\n",
    "    statics_dict[current_year] = (len(triads) // 2, num_of_new_triadic_closure, len(current_triadic_closure_set) // 2)\n",
    "    print()\n",
    "    current_year += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(get_hashcodes('gethashcode'))\n",
    "count = 0\n",
    "author_hashcodes = dict()\n",
    "for authorships in coauthors_all:\n",
    "    for author in authorships:\n",
    "        if author not in author_hashcodes:\n",
    "            hashcodes = get_hashcodes(author)\n",
    "            author_hashcodes[author] = hashcodes \n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(count)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_size = 680\n",
    "dimensions = [[680, 680], [680, 680], [680, 680], [680, 680], [680, 680],[620, 749],[680, 680], [680, 680],\n",
    "              [749, 620], [580, 797], [797, 580], [520, 889], [889, 520],[480, 963],[963, 480], [680, 680]]\n",
    "sketch_no = len(dimensions)\n",
    "sketch = []\n",
    "for s_no in range(sketch_no):\n",
    "    sketch.append(np.zeros((dimensions[s_no][0], dimensions[s_no][1]), dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq1 = PriorityQueue()\n",
    "name_set1 = set()\n",
    "\n",
    "pq2 = PriorityQueue()\n",
    "pq22 = PriorityQueue()\n",
    "name_set2 = set()\n",
    "\n",
    "topk = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build sketch\n",
    "count = 0\n",
    "edge_count = 0\n",
    "for coauthorship in coauthors_all:\n",
    "    for i in range(len(coauthorship)):\n",
    "        for j in range(len(coauthorship)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            edge_count += 1\n",
    "            author1 = coauthorship[i]\n",
    "            author2 = coauthorship[j]\n",
    "            hashcodes1 = author_hashcodes[author1]\n",
    "            hashcodes2 = author_hashcodes[author2]\n",
    "            for s_no in range(sketch_no):\n",
    "                hashcode1 = hashcodes1[s_no] % dimensions[s_no][0]\n",
    "                hashcode2 = hashcodes2[s_no] % dimensions[s_no][1]\n",
    "                sketch[s_no][hashcode1][hashcode2] += 1\n",
    "                \n",
    "            #query frequencies\n",
    "            frequency = sys.maxsize\n",
    "            for s_no in range(sketch_no):\n",
    "                hashcode1 = hashcodes1[s_no] % dimensions[s_no][0]\n",
    "                hashcode2 = hashcodes2[s_no] % dimensions[s_no][1]\n",
    "                frequency = min(sketch[s_no][hashcode1][hashcode2], frequency)\n",
    "                \n",
    "            if author1 == 'Michael H. Böhlen':\n",
    "                if author2 in name_set1:\n",
    "                    pq11 = PriorityQueue()\n",
    "                    while pq1.qsize() > 0:\n",
    "                        fre, name = pq1.get()\n",
    "                        if name != author2:\n",
    "                            pq11.put((fre, name))\n",
    "                        else:\n",
    "                            pq11.put((frequency, author2))\n",
    "                    pq1 = pq11\n",
    "                else:\n",
    "                    if pq1.qsize() < topk:\n",
    "                        pq1.put((frequency, author2))\n",
    "                        name_set1.add(author2)\n",
    "                    else: \n",
    "                        frequency_in_queue, author_in_queue = pq1.get()\n",
    "                        if(frequency_in_queue < frequency):\n",
    "                            pq1.put((frequency, author2))\n",
    "                            name_set1.remove(author_in_queue)\n",
    "                            name_set1.add(author2)\n",
    "                        else:\n",
    "                            pq1.put((frequency_in_queue, author_in_queue))\n",
    "                    \n",
    "            elif author1 == 'Johann Gamper':\n",
    "                if author2 in name_set2:\n",
    "                    pq22 = PriorityQueue()\n",
    "                    while pq2.qsize() > 0:\n",
    "                        fre, name = pq2.get()\n",
    "                        if name != author2:\n",
    "                            pq22.put((fre, name))\n",
    "                        else:\n",
    "                            pq22.put((frequency, author2))\n",
    "                    pq2 = pq22\n",
    "                else:\n",
    "                    if pq2.qsize() < topk:\n",
    "                        pq2.put((frequency, author2))\n",
    "                        name_set2.add(author2)\n",
    "                    else: \n",
    "                        frequency_in_queue, author_in_queue = pq2.get()\n",
    "                        if(frequency_in_queue < frequency):\n",
    "                            pq2.put((frequency, author2))\n",
    "                            name_set2.remove(author_in_queue)\n",
    "                            name_set2.add(author2)\n",
    "                        else:\n",
    "                            pq2.put((frequency_in_queue, author_in_queue))     \n",
    "                            \n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(count)\n",
    "print(print('edge count: %d' %edge_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build sketch\n",
    "target1 = 'Michael H. Böhlen'\n",
    "target2 = 'Johann Gamper'\n",
    "name_set = set()\n",
    "pq_CT = PriorityQueue()\n",
    "topk = 10\n",
    "count = 0\n",
    "edge_count = 0\n",
    "\n",
    "for coauthorship in coauthors_all:\n",
    "    for i in range(len(coauthorship)):\n",
    "        for j in range(len(coauthorship)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            edge_count += 1\n",
    "            author1 = coauthorship[i]\n",
    "            author2 = coauthorship[j]\n",
    "            hashcodes1 = author_hashcodes[author1]\n",
    "            hashcodes2 = author_hashcodes[author2]\n",
    "            for s_no in range(sketch_no):\n",
    "                hashcode1 = hashcodes1[s_no] % dimensions[s_no][0]\n",
    "                hashcode2 = hashcodes2[s_no] % dimensions[s_no][1]\n",
    "                sketch[s_no][hashcode1][hashcode2] += 1\n",
    "                \n",
    "            if (author1 == target1 and author2 != target2) or (author1 == target2 and author2 != target1):\n",
    "                 #query frequencies\n",
    "                frequency1 = sys.maxsize\n",
    "                target1_hashcodes = author_hashcodes[target1]\n",
    "                for s_no in range(sketch_no):\n",
    "                    hashcode1 = target1_hashcodes[s_no] % dimensions[s_no][0]\n",
    "                    hashcode2 = hashcodes2[s_no] % dimensions[s_no][1]\n",
    "                    frequency1 = min(sketch[s_no][hashcode1][hashcode2], frequency1)\n",
    "                \n",
    "                frequency2 = sys.maxsize\n",
    "                target2_hashcodes = author_hashcodes[target2]\n",
    "                for s_no in range(sketch_no):\n",
    "                    hashcode1 = target2_hashcodes[s_no] % dimensions[s_no][0]\n",
    "                    hashcode2 = hashcodes2[s_no] % dimensions[s_no][1]\n",
    "                    frequency2 = min(sketch[s_no][hashcode1][hashcode2], frequency2)\n",
    "                    \n",
    "                if frequency1 <= 0 or frequency2 <= 0:\n",
    "                    continue\n",
    "                \n",
    "                frequency = (frequency1 * frequency2) // (frequency1 + frequency2)\n",
    "                \n",
    "                if author2 in name_set:\n",
    "                    pq_temp = PriorityQueue()\n",
    "                    while pq_CT.qsize() > 0:\n",
    "                        fre, name = pq_CT.get()\n",
    "                        if name != author2:\n",
    "                            pq_temp.put((fre, name))\n",
    "                        else:\n",
    "                            pq_temp.put((frequency, author2))\n",
    "                    pq_CT = pq_temp\n",
    "                else:\n",
    "                    if pq_CT.qsize() < topk:\n",
    "                        pq_CT.put((frequency, author2))\n",
    "                        name_set.add(author2)\n",
    "                    else: \n",
    "                        frequency_in_queue, author_in_queue = pq_CT.get()\n",
    "                        if(frequency_in_queue < frequency):\n",
    "                            pq_CT.put((frequency, author2))\n",
    "                            name_set.remove(author_in_queue)\n",
    "                            name_set.add(author2)\n",
    "                        else:\n",
    "                            pq_CT.put((frequency_in_queue, author_in_queue))\n",
    "                            \n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print(count)\n",
    "print('edge count: %d' %edge_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(sketch)\n",
    "author1 = 'Michael H. Böhlen'\n",
    "author2 = 'Johann Gamper'\n",
    "sketch_indices1 = [ i % sketch_size for i in author_hashcodes[author1]]\n",
    "sketch_indices2 = [ i % sketch_size for i in author_hashcodes[author2]]\n",
    "topk = 5\n",
    "\n",
    "topIndices1 = []\n",
    "topIndices2 = []\n",
    "for s in range(sketch_no):\n",
    "    index1 = sketch_indices1[s]\n",
    "    coauthors_weight1 = sketch[s][index1]\n",
    "    topk_index1 = sorted(range(len(coauthors_weight1)), key=lambda i: coauthors_weight1[i])[-topk:]\n",
    "    topIndices1.append(topk_index1)\n",
    "    \n",
    "    index2 = sketch_indices2[s]\n",
    "    coauthors_weight2 = sketch[s][index2]\n",
    "    topk_index2 = sorted(range(len(coauthors_weight2)), key=lambda i: coauthors_weight2[i])[-topk:]\n",
    "    topIndices2.append(topk_index2)\n",
    "    \n",
    "    print(topk_index1)\n",
    "    print(topk_index2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq = PriorityQueue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pq2.queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pq1.queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pq_CT.queue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
